<!doctype html><html lang=en><head><script async src=//static.getclicky.com/101389223.js></script><noscript><p><img alt=Clicky width=1 height=1 src=//in.getclicky.com/101389223ns.gif></p></noscript><title>Converting a Kaggle notebook to a Kedro Pipeline</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="origin"><meta name=description content="For the fine bits between the pickles Adding nutricious solutions to tech teams and R&D projects "><meta name=generator content="Hugo 0.145.0"><link rel=stylesheet href=/css/main.min.0d3ff0edd6afcf5ad44950b3254e74e61e213c66e3b539f584bbc0053d643bca9c189bfafc81af238096129aeae53f90cd63a87a45b6eb37395a3db02c09643c.css integrity="sha512-DT/w7davz1rUSVCzJU505h4hPGbjtTn1hLvABT1kO8qcGJv6/IGvI4CWEprq5T+QzWOoekW26zc5Wj2wLAlkPA=="><noscript><link rel=stylesheet href=/css/noscript.min.e6f1ba19697eecfddfbf83ff7181b98181998f163d7005f6ae923451556bf85bef357f43dffe1522b92c1efab7fb38441f479e39b7a03e4313a8ef12b0b01f65.css integrity="sha512-5vG6GWl+7P3fv4P/cYG5gYGZjxY9cAX2rpI0UVVr+FvvNX9D3/4VIrksHvq3+zhEH0eeObegPkMTqO8SsLAfZQ=="></noscript><meta name=twitter:card content="summary"><meta name=twitter:title content="Converting a Kaggle notebook to a Kedro Pipeline"><meta name=twitter:description content="Get a machine learning notebook ready for production by converting it to a Kedro pipeline."><meta property="og:url" content="https://picklejuicedev.github.io/posts/notebook_2_kedro/"><meta property="og:site_name" content="picklejuice"><meta property="og:title" content="Converting a Kaggle notebook to a Kedro Pipeline"><meta property="og:description" content="Get a machine learning notebook ready for production by converting it to a Kedro pipeline."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-09T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-09T00:00:00+00:00"><meta property="article:tag" content="Data"><meta property="article:tag" content="Kedro"></head><body class="landing is-preload"><div id=page-wrapper><header id=header><h1><a href=https://picklejuicedev.github.io/><img src=../../images/icon_dark.png width=35 style=vertical-align:middle>
<span style=vertical-align:top>picklejuice</span></a></h1><nav id=nav><ul><li class=special><a href=#menu class=menuToggle><span>Menu</span></a><div id=menu><ul><li><a href=/>Home</a></li><li><a href=/mkdocmaker/>MkDoc Maker</a></li><li><a href=/posts/>Posts</a></li><li><a href=/aboutme/>About</a></li></ul></div></li></ul></nav></header><article id=main><header><h2>Converting a Kaggle notebook to a Kedro Pipeline</h2></header><section class="wrapper style5"><div class=inner><p>This tutorial is aimed at Data Scientists who want to convert Jupyter Notebook to a Kedro pipeline. Notebooks are a great way to experiment and explore data, figure out what machine learning model works well and illustrate outcomes. But it is very hard to maintain notebooks, they don&rsquo;t play nice with repositories, testing can&rsquo;t be automated and you can&rsquo;t reuse code from them or integrate them into a bigger workflow. So Kedro takes all these good things we have learned from software engineering and applies them to data science projects. But how to get started?</p><p>We will be using a Kaggle notebook project as example and convert it to a Kedro pipeline. Along the way I will aim to illustrate the steps you should consider and why they are important.</p><h2 id=prerequisites>Prerequisites</h2><p>If you want to follow along you need to install Jupyter Notebook or Jupyter Lab as well as Kedro. All the examples use VS Code. An ideal starting point is the end of my tutorial on <a href=/posts/quick_kedro_setup/>Setting up a Virtual Environment and VS Code for Kedro</a> . It guides you through from start to having a blank, working Kedro project. The examples are from a Windows environment, so there may be subtle differences for Mac users. I have also prepared a branch with the output from this tutorial which you can clone using:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>git clone --branch empty_kedro_project https<span style=color:#960050;background-color:#1e0010>:</span>//github.com/picklejuicedev/kedro-tutorial.git
</span></span></code></pre></div><p>Remember to create a virtual environment and install all the dependencies using:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>pip install -r src/requirements.txt
</span></span></code></pre></div><h2 id=the-notebook>The notebook</h2><p>If you like, please download this <a href=/data/10_Minimal_Spaceship.ipynb download>notebook</a> and the <a href=/data/train.csv download>data</a>. Feel free to explore, it is a pretty basic ML project, taken from the <a href=https://www.kaggle.com/competitions/spaceship-titanic>Kaggle starter competition &ldquo;Spaceship Titanic&rdquo;</a>.</p><p>Initially it will install the missing <code>pandas</code> and <code>scikit-learn</code> packages. After loading the data, we do some pre-processing, figuring out if a passenger was travelling alone or in a group. Next, we take care of <code>NaN</code> values. Clearly there is more pre-processing we could do, but let&rsquo;s keep it simple.</p><p>We then use a <code>LabelEncoder</code> to convert some label columns in to integer values and delete uneccessary columns.</p><p>Next we split the data into training and test sets and train a <code>RandomForrestClassifier</code>.</p><p>Last but not least we print a report with how it is performing.</p><p>If you run this notebook it will create some results like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>---------- RandomForest ----------
</span></span><span style=display:flex><span>Accuracy = <span style=color:#ae81ff>77.84</span> %
</span></span><span style=display:flex><span>Precision = <span style=color:#ae81ff>78.11</span> %
</span></span><span style=display:flex><span>Recall = <span style=color:#ae81ff>75.65</span> %
</span></span><span style=display:flex><span>F1-Score = <span style=color:#ae81ff>76.86</span> %
</span></span></code></pre></div><h2 id=analysing-the-structure>Analysing the structure</h2><p>Before creating a Kedro project, let&rsquo;s establish what the different parts of the pipeline would be. A good starting point is:</p><ul><li>Pre-processing</li><li>Encoding</li><li>Make a model</li><li>Reporting</li></ul><p>I have already annotated the notebook with some of those functions, but we want to create processing blocks or nodes that do each task. This way we can</p><ul><li>test and debug each task</li><li>they have a clearly defined input and output (or interface)</li><li>which means we can then easily swap them out with alternative processing blocks</li><li>connect building blocks to create a pipeline<ul><li>that can run locally as well as remotely</li><li>can be triggered or scheduled</li><li>can be fed different inputs if we need to update the model</li></ul></li></ul><p>So these and more are the benefits when moving to Kedro.</p><h2 id=project-setup>Project setup</h2><p>Let&rsquo;s start with a blank Kedro project, if you need any help, check out the <a href=https://kedro.readthedocs.io/en/stable/get_started/new_project.html>Kedro documentation</a> or my article on <a href=/posts/quick_kedro_setup/>Setting up a Virtual Environment and VS Code for Kedro</a> .</p><p>As a starting point we want to copy 2 files into the project folder:</p><ul><li>first, copy <code>train.csv</code> to <code>data/01_raw/</code></li><li>then, copy the notebook into the folder <code>notebooks/</code></li></ul><p>We need to install the <code>pandas</code> and <code>scikit-learn</code> packages that will be used by the project:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>pip install pandas scikit-learn
</span></span></code></pre></div><p>Next, open up VS code if not already running. Be sure to have the virtual environment with all dependencies as interpreter.</p><h3 id=registering-the-data-file>Registering the data file</h3><p>Before we register the csv file in the project catalog, let&rsquo;s look at the reasoning behind having a YAML file for this. After all, we could just reference it directly as in the notebook, right? But let&rsquo;s imagine moving this project to a cloud server, then the file reference will change as the file may be located somewhere else. Or the name changes. Or you are given a parquet file instead of a csv file. In all these cases you would simply edit the <code>catalog.yml</code> file and don&rsquo;t need to touch any code. This makes it easier to adapt and less likely to break. For the same reason user configurable parameters are saved in the <code>parameters.yml</code> file. Or logging details in <code>logging.yml</code>. I think you get the idea, it simply makes good sense to move anything that frequently changes into a settings file so it can be modified without updating the code.</p><p>Open <code>conf/base/catalog.yml</code> and add:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e>#catalog.yml</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>train</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>pandas.CSVDataSet</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>filepath</span>: <span style=color:#ae81ff>data/01_raw/train.csv</span>
</span></span></code></pre></div><p>Let&rsquo;s load this now in our notebook. As we are in VS Code, we can open a new integrated terminal (right click in explorer), then use the <code>kedro jupyter lab</code> command to start the notebook editor. Once inside, change the loading of the data from using <code>pandas</code> to <code>catalog.load</code>:</p><p><img src=/images/LaunchNotebook.gif alt="Launching Notebook"></p><p>Please note that if the catalog doesn&rsquo;t load then you need to change the kernel for the notebook to the one labelled &ldquo;Kedro(kedro_project)&rdquo;. Kedro &ldquo;injects&rdquo; the catalog and other variables into the kernel so they are accessible by the notebook. Whilst VS Code can run Jupyter Notebooks inside the editor there can be problems with injecting the Kedro variables. So we&rsquo;ll stick to running notebooks the classic way.</p><p>To shut down the Jupyter lab server, press <code>crtl + C</code> twice in the terminal window. For now, just leave it running as we will move some code around.</p><h2 id=converting-code-blocks-into-functions>Converting code blocks into functions</h2><p>We can now create a new pipeline in Kedro and start moving the code from the Jupyter Cells into dedicated functions which will become nodes. Let&rsquo;s go through the steps first and then discuss why this is useful after.</p><p>A pipeline is a data processing flow, it takes input datasets like our csv file and passes it through various nodes, each doing one step of the processing, until you have an output at the end. In our case it will simply print the results to the log.</p><p>Let&rsquo;s create a pipeline called <code>data_proc</code> in the command line:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>kedro pipeline create data_proc
</span></span></code></pre></div><p>This will create empty files for us to populate. Open <code>src/kedro_project/pipelines/data_proc/nodes.py</code>.</p><p>We will take the two pre-processing cells from the notebook and copy them over, then make functions out of them:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># pipelines/data_proc/nodes.py</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_group_size</span>(df_train: pd<span style=color:#f92672>.</span>DataFrame) <span style=color:#f92672>-&gt;</span> pd<span style=color:#f92672>.</span>DataFrame:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        df_train (pandas.DataFrame): Dataframe with training data.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        pandas.DataFrame: Dataframe with training data and group size.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    split_df <span style=color:#f92672>=</span> df_train[<span style=color:#e6db74>&#34;PassengerId&#34;</span>]<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>split(pat<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;_&#34;</span>, expand<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    alone <span style=color:#f92672>=</span> split_df[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>value_counts()
</span></span><span style=display:flex><span>    split_df <span style=color:#f92672>=</span> split_df<span style=color:#f92672>.</span>merge(alone<span style=color:#f92672>.</span>rename(<span style=color:#e6db74>&#34;groupSize&#34;</span>), left_on<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, right_index<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    df_train[<span style=color:#e6db74>&#34;groupSize&#34;</span>] <span style=color:#f92672>=</span> split_df[<span style=color:#e6db74>&#34;groupSize&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df_train
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>resolve_na</span>(df_train: pd<span style=color:#f92672>.</span>DataFrame) <span style=color:#f92672>-&gt;</span> pd<span style=color:#f92672>.</span>DataFrame:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        df_train (pandas.DataFrame): Dataframe with training data.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        pandas.DataFrame: Dataframe with training data and filled na.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># deal with Nan: fill costs with 0.0, remove all others</span>
</span></span><span style=display:flex><span>    cols <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;RoomService&#34;</span>, <span style=color:#e6db74>&#34;FoodCourt&#34;</span>, <span style=color:#e6db74>&#34;ShoppingMall&#34;</span>, <span style=color:#e6db74>&#34;Spa&#34;</span>, <span style=color:#e6db74>&#34;VRDeck&#34;</span>]
</span></span><span style=display:flex><span>    df_train[cols] <span style=color:#f92672>=</span> df_train[cols]<span style=color:#f92672>.</span>fillna(<span style=color:#ae81ff>0.0</span>)
</span></span><span style=display:flex><span>    df_train <span style=color:#f92672>=</span> df_train<span style=color:#f92672>.</span>dropna()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df_train
</span></span></code></pre></div><p>There is quite a bit of code, let&rsquo;s step through it:</p><ul><li>function names (i.e. <code>compute_group_size</code>) - creating functions is all about hiding detail. The name gives you a vital idea what happens in it. So you don&rsquo;t need to read the code anymore.</li><li>type hints (i.e. <code>pd.DataFrame</code>) - informs the reader of the function what is expected as input and output of the function. Here it expects a pandas dataframe as input and also returns one.</li><li>doc_string (i.e comments after the function name) - annotations explaining the usage of the function. If you stick to this format, VS Code or other IDEs finds them and can display them as hints.</li><li>return (i.e. <code>df_train</code>) - the output of the function that will be returned to the caller.</li></ul><p>Good functions have a clear name, do only one thing and it is very clear what they do. You feed something in and get something out. This is a key concept in software engineering: hiding complexity.</p><p>This brings me to the topic of automating the process of conversion of Notebooks to plain python code. They do work, but you need to take extra care when writing the notebook in the first place. For me this defeats the purpose of using notebooks, I like to be really messy and just explore. During the process of converting the code to functions is when they will be tidied up, reviewed and documented. This is very much a hands-on approach.</p><h2 id=building-the-pipeline>Building the pipeline</h2><p>So we have built some nodes, but not yet used them. Let&rsquo;s import them into <code>pipelines/data_proc/pipeline.py</code> and add them to the pipeline:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># pipelines/data_proc/pipeline.py</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> kedro.pipeline <span style=color:#f92672>import</span> Pipeline, node, pipeline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> .nodes <span style=color:#f92672>import</span> compute_group_size, resolve_na
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_pipeline</span>(<span style=color:#f92672>**</span>kwargs) <span style=color:#f92672>-&gt;</span> Pipeline:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pipeline(
</span></span><span style=display:flex><span>        [
</span></span><span style=display:flex><span>            node(
</span></span><span style=display:flex><span>                func<span style=color:#f92672>=</span>compute_group_size, 
</span></span><span style=display:flex><span>                inputs<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>, 
</span></span><span style=display:flex><span>                outputs<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train_with_group_size&#34;</span>
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            node(
</span></span><span style=display:flex><span>                func<span style=color:#f92672>=</span>resolve_na,
</span></span><span style=display:flex><span>                inputs<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train_with_group_size&#34;</span>,
</span></span><span style=display:flex><span>                outputs<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train_preprocessed&#34;</span>,
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>In here, we import the nodes from the <code>.nodes</code> file and then declare the inputs and outputs for each node.</p><p>Once done, why not try and run the pipeline with the <code>kedro run</code> command in the terminal:</p><p><img src=/images/pipeline_run.png alt=image-20221207160746020></p><p>Looks like our pipeline is working. And here is another important building block for a production pipeline: A good logger! Often your code will run on different platforms, in the cloud, on a server or a machine from a colleague. If there is a problem, a log can give you vital information what is going wrong and how to fix it. You can find a saved version of this log in <code>logs/info.logs</code>.</p><h2 id=combining-pipelines>Combining pipelines</h2><p>Feel free to carry on adding and converting your own pipelines for a bit of a challenge. Or download the completed project from my GitHub repository:</p><pre tabindex=0><code>git clone --branch notebook2kedro https://github.com/picklejuicedev/kedro-tutorial.git
</code></pre><p>In total I created 4 pipelines:</p><ul><li><code>data_proc</code>: for pre-processing the training data</li><li><code>data_encode</code>: for encoding the string based enumerations</li><li><code>data_model</code>: for creating the model</li><li><code>data_report</code>: for printing some performance metrics of the model</li></ul><p>Each pipeline has a clear input and output. Now the nice thing about Kedro is that you don&rsquo;t need to tell it which pipeline to run when. You connect them together by specifying inputs that are connected to outputs. For example the data_model pipeline takes the <code>train_encoded</code> dataset as input and returns a <code>model</code>, <code>X_test</code> and <code>y_test</code>. You can persist any dataset to storage by providing an entry for it in the <code>catalog.yml</code> file. So to keep the model, I added this entry to save it as pickle dataset:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>model</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>pickle.PickleDataSet</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>filepath</span>: <span style=color:#ae81ff>data/06_models/model.pickle</span>
</span></span></code></pre></div><p>As before you can now run the complete pipeline with <code>kedro run</code> and you should see a finished model appear in the folder <code>data/06_models</code>.</p><h2 id=visualising-our-pipelines>Visualising our pipelines</h2><p>The last we will do in this tutorial is have a look at the pipeline we created using Kedro-Viz. First install it with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install kedro-viz
</span></span></code></pre></div><p>Then use <code>kedro viz</code> to run it. You can now explore the different pipelines, datasets and nodes, even their implementation as code:</p><p><img src=/images/LaunchKedroViz.gif alt=LaunchKedroViz></p><h2 id=conclusion>Conclusion</h2><p>There you have it, one notebook converted into a Kedro pipeline that now provides the basis to take it to the next level. How about improving the model and creating an alternative path for some of the nodes? Or rewriting the reporting node to create some nice graphs instead of a simple console print out. Any of those changes can now be safely done because we have clearly defined processing blocks and interfaces between them. So as long as they take the same inputs and outputs you can easily create a new one.</p><p>Plus we can also consider some of the other software development best practices, such as automated testing to ensure the nodes and pipelines keep working the way we have designed them. We can also save this project to GitHub and share with colleagues. If we have kept the <code>requirements.txt</code> up to date, then they can run the same project and contribute. Or we can look at deploying our project to a cloud provider to rebuild our model automatically under certain conditions, maybe when new input data has arrived.</p><p>If you enjoyed this tutorial or have any suggestions for further Kedro tutorials, please leave a comment. Maybe some automated testing next?</p></div></section></article><footer id=footer><ul class=icons><li><a href=mailto:sean@pickle-juice.co.uk class="icon fa-envelope-o"><span class=label>Email</span></a></li></ul><ul class=copyright><li>&copy; 2025 picklejuice</li><li>Design: <a href=http://html5up.net>HTML5 UP</a></li></ul></footer></div><script src=/js/bundle.min.f09ff27f2095da4de8a286f6c80a96dc2020bd58ce449034e4eee9390e0067eed12ca38ae93650d89c297de367e218ef2c21c2f45e08913f1f684a2fe4c26345.js integrity="sha512-8J/yfyCV2k3ooob2yAqW3CAgvVjORJA05O7pOQ4AZ+7RLKOK6TZQ2JwpfeNn4hjvLCHC9F4IkT8faEov5MJjRQ=="></script></body></html>